\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
% \usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{array}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{textcomp}
\usepackage{float}
\usepackage{balance}
\usepackage{multirow}
\usepackage{xspace}
\usepackage[htt]{hyphenat}
\usepackage{xcolor}
\usepackage{caption}
\usepackage[most]{tcolorbox}
\usepackage{booktabs}
\usepackage{listings}
\usepackage{tikz}
\usepackage{makecell}
\usepackage{subcaption}
\usepackage{ragged2e}                       % new
\usepackage{booktabs, makecell, multirow, tabularx}


\lstset{
    basicstyle=\ttfamily,  % Basic font style
    keywordstyle=\color{blue}, % Keywords color
    commentstyle=\color{green}, % Comments color
    stringstyle=\color{red}, % Strings color
    tabsize=4,            % Tab size
    showspaces=false,      % Show spaces in code
    showstringspaces=false, % Show spaces in strings
    breaklines=true,       % Enable line breaking
}


\PassOptionsToPackage{hyphens}{url}\usepackage{hyperref}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\newcommand{\name}{\textcolor{black}{Installamatic}\xspace}

\makeatletter
\newcommand\notsotiny{\@setfontsize\notsotiny\@vipt\@viipt}
\makeatother

\begin{document}

\title{Appendix for \\Beyond pip install: Evaluating LLM agents for the automated installation of Python projects}

\onecolumn
\maketitle
    In this document, we provide supplementary materials to the main content of the paper. This includes the prompts used by \name, and three tables:
    the first provides a brief example of the type of commands in the exemplar Dockerfiles that would warrant assigning each tag;
    the second table shows detailed results for the first experiment conducted,
    in which \name is tasked with installing the repositories in the dataset after performing the document gathering step;
    Finally, the third table shows the results for the second experiment, in which the document gathering step is skipped,
    and all install-relevant documents are provided to the \name automatically.
    The second experiment excludes the \texttt{icloud-drive-docker}, \texttt{instructor}, \texttt{yfinance}, \texttt{tqdm}, \texttt{core} and \texttt{sherlock} repositories.
    This is due to the fact that these repositories do not contain any install-relevant information, and therefore the perfect recall scenario would not apply to them.

\section{Description and Example of Each Tag}
\begin{table}[!h]
\centering
\begin{tabular}{|lp{0.3\textwidth}l|}
\hline
\multicolumn{1}{|l|}{\textbf{Tag}}       & \multicolumn{1}{p{0.4\textwidth}|}{\textbf{Description}} 
& Example \\ \hline
\textbf{Installation}                    &  &  \\ \hline
\multicolumn{1}{|l|}{requirements}       & \multicolumn{1}{p{0.4\textwidth}|}{Installation of dependencies using \texttt{pip install -r requirements.txt}.} 
    & \begin{lstlisting}
RUN pip install -r requirements.txt
    \end{lstlisting}   \\ \hline
\multicolumn{1}{|l|}{requirements-extra} & \multicolumn{1}{p{0.4\textwidth}|}{Installation of dependencies from additional requirements.} 
    & \begin{lstlisting}
RUN pip install -r test_requirements.txt
    \end{lstlisting} \\ \hline
\multicolumn{1}{|l|}{pip-extra}          & \multicolumn{1}{p{0.4\textwidth}|}{Requiring the use of pip to install of something other than Poetry or the contents of a requirements file.}
    & \begin{lstlisting}
RUN pip install requests_cache requests_ratelimiter
    \end{lstlisting} \\ \hline
\multicolumn{1}{|l|}{poetry}             & \multicolumn{1}{p{0.4\textwidth}|}{Installation of dependencies using the Poetry dependency manger} 
    & \begin{lstlisting}
RUN pip install poetry
RUN poetry install
RUN poetry run ...
    \end{lstlisting} \\ \hline
\multicolumn{1}{|l|}{poetry-extra}       & \multicolumn{1}{p{0.4\textwidth}|}{Installation of dependencies using Poetry, with additional arguments.} 
    & \begin{lstlisting}
RUN pip install poetry
RUN poetry install --with dev,docs -E all
RUN poetry run ...        
    \end{lstlisting} \\ \hline
\multicolumn{1}{|l|}{make-install}       & \multicolumn{1}{p{0.4\textwidth}|}{Installation of dependencies using a makefile, typically commands such as \texttt{make install} or \texttt{make init}.} 
    & \begin{lstlisting}
RUN make init
    \end{lstlisting}\\ \hline
\multicolumn{1}{|l|}{install-self}       & \multicolumn{1}{p{0.4\textwidth}|}{Achieved by running \texttt{pip install -e .}, this means that the project itself needs to installed in the working environment in order for tests to run.}
    & \begin{lstlisting}
RUN pip install --no-build-isolation --editable .
    \end{lstlisting} \\ \hline
\multicolumn{1}{|l|}{install-pytest}     & \multicolumn{1}{p{0.4\textwidth}|}{The Pytest library needs to be installed manually.} 
    & \begin{lstlisting}
RUN pip install pytest
    \end{lstlisting} \\ \hline
\multicolumn{1}{|l|}{install-tox}        & \multicolumn{1}{p{0.4\textwidth}|}{The Tox library needs to be installed manually.} 
    & \begin{lstlisting}
RUN pip install tox
    \end{lstlisting} \\ \hline
\multicolumn{1}{|l|}{install-other}      & \multicolumn{1}{p{0.4\textwidth}|}{Perform installation of dependencies through other means, such as a custom script contained in the repository}
    & \begin{lstlisting}
RUN scripts/install
    \end{lstlisting} \\ \hline
\textbf{Testing}                         &                                                                                                                                                                                                  &         \\ \hline
\multicolumn{1}{|l|}{pytest}             & \multicolumn{1}{p{0.4\textwidth}|}{Tests are run using PyTest.}
    & \begin{lstlisting}
RUN pytest
    \end{lstlisting} \\ \hline
\multicolumn{1}{|l|}{pytest-extra}       & \multicolumn{1}{p{0.4\textwidth}|}{Additional arguments need to be provided to pytest, such as specifying the location of the tests or additional flags.} 
& \begin{lstlisting}
RUN poetry run pytest --fast-test-mode.
\end{lstlisting} \\ \hline
\multicolumn{1}{|l|}{tox} & \multicolumn{1}{p{0.4\textwidth}|}{Tests are run using Tox.}
    & \begin{lstlisting}
RUN tox
    \end{lstlisting} \\ \hline
\multicolumn{1}{|l|}{unittest}           & \multicolumn{1}{p{0.4\textwidth}|}{Tests are run using Python's built in unnittest command.} 
    & \begin{lstlisting}
RUN python -m unittest discover
    \end{lstlisting} \\ \hline
\multicolumn{1}{|l|}{make-test}          & \multicolumn{1}{p{0.4\textwidth}|}{Tests are run using a makefile with a command such as \texttt{make test}.}
    & \begin{lstlisting}
RUN make test
    \end{lstlisting} \\ \hline
\multicolumn{1}{|l|}{test-other}         & \multicolumn{1}{p{0.4\textwidth}|}{Tests are run some other way, such as a \texttt{test.py} file.} 
    & \begin{lstlisting}
RUN python setup.py test
    \end{lstlisting}\\ \hline
\textbf{Other}                           &                                                                                                                                                                                                  &         \\ \hline
\multicolumn{1}{|l|}{bash-extra}         & \multicolumn{1}{p{0.4\textwidth}|}{Requiring additional bash commands to set up the repository, such as creating new directories or granting permissions to certain files.}
& \begin{lstlisting}

ENV OPENAI_API_KEY=x    
\end{lstlisting}\\ \hline
\end{tabular}
\caption{Description and example of each installation tag}
\end{table}

\clearpage
\section{Experiment Results}

\begin{table}[!h]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|}
    \hline
        \textbf{Repository} & \textbf{Build Rate} & \textbf{Average \#Attempts} & \textbf{Average Duration (s)} & \textbf{Average Recall} & \textbf{Average \#Relevant} & \textbf{Average \#Irrelevant} \\ \hline
        mypy & 9/10 & 1.6 & 898.821 & 0.9 & 1 & 2.4 \\ \hline
        Torch-Pruning & 0/10 & 2.5 & 405.783 & 1.0 & 1 & 1.5 \\ \hline
        scapy & 3/10 & 2.0 & 263.935 & 0.25 & 2 & 1.4 \\ \hline
        ydata-profiling & 2/10 & 2.6 & 676.187 & 0.45 & 2 & 2.4 \\ \hline
        cloud-custodian & 0/10 & 3.0 & 197.62 & 0.0 & 1 & 1.0 \\ \hline
        black & 4/10 & 2.4 & 317.092 & 0.0 & 1 & 1.8 \\ \hline
        speechbrain & 0/10 & 2.3 & 504.869 & 0.35 & 2 & 2.6 \\ \hline
        camel & 0/10 & 2.5 & 250.867 & 0.5 & 3 & 0.1 \\ \hline
        open-interpreter & 8/10 & 1.9 & 200.01 & 0.7 & 1 & 0.6 \\ \hline
        sabnzbd & 0/10 & 2.5 & 297.514 & 0.9 & 1 & 1.1 \\ \hline
        sherlock & 1/10 & 2.6 & 113.326 & 0.0 & 0 & 1.0 \\ \hline
        pymc & 0/10 & 2.8 & 808.95 & 0.0 & 2 & 2.7 \\ \hline
        pennylane & 0/10 & 2.4 & 246.368 & 0.033 & 3 & 2.2 \\ \hline
        beets & 7/8 & 2.25 & 129.139 & 0.875 & 1 & 0.5 \\ \hline
        instructor & 0/10 & 2.9 & 235.4 & 0.0 & 0 & 2.2 \\ \hline
        scvi-tools & 0/10 & 2.5 & 912.604 & 0.3 & 1 & 1.7 \\ \hline
        boto3 & 0/10 & 3.0 & 1090.803 & 0.8 & 1 & 1.3 \\ \hline
        tqdm & 4/10 & 2.3 & 254.542 & 0.0 & 0 & 2.2 \\ \hline
        moto & 6/10 & 1.5 & 1470.263 & 0.233 & 3 & 2.3 \\ \hline
        X-AnyLabeling & 2/10 & 2.4 & 282.206 & 0.2 & 1 & 2.5 \\ \hline
        fastapi & 7/10 & 1.9 & 150.565 & 0.0 & 2 & 2.3 \\ \hline
        sympy & 0/10 & 2.6 & 3009.718 & 0.5 & 2 & 1.0 \\ \hline
        yfinance & 0/10 & 2.8 & 139.954 & 0.0 & 0 & 2.1 \\ \hline
        R2R & 0/10 & 1.7 & 210.39 & 0.2 & 1 & 1.5 \\ \hline
        rich & 7/10 & 2.3 & 118.019 & 1.0 & 1 & 0.1 \\ \hline
        numba & 0/10 & 2.5 & 145.67 & 0.0 & 2 & 2.1 \\ \hline
        dlt & 0/10 & 2.9 & 452.216 & 1.0 & 1 & 0.5 \\ \hline
        aim & 0/10 & 2.3 & 348.748 & 1.0 & 1 & 0.9 \\ \hline
        qlib & 0/10 & 2.5 & 775.61 & 0.5 & 2 & 0.9 \\ \hline
        textual & 10/10 & 1.7 & 387.686 & 1.0 & 1 & 0.0 \\ \hline
        nonebot2 & 0/10 & 3.0 & 249.332 & 0.2 & 1 & 1.0 \\ \hline
        opencompass & 1/10 & 2.7 & 568.671 & 0.5 & 2 & 0.5 \\ \hline
        django-stubs & 8/10 & 2.1 & 291.474 & 1.0 & 1 & 1.1 \\ \hline
        you-get & 8/10 & 2.1 & 106.523 & 0.8 & 1 & 1.6 \\ \hline
        spotify-downloader & 9/10 & 1.6 & 329.153 & 0.5 & 2 & 1.8 \\ \hline
        core & 0/10 & 2.0 & 365.041 & 0.0 & 0 & 2.3 \\ \hline
        starlette & 8/10 & 1.7 & 98.334 & 0.4 & 2 & 2.0 \\ \hline
        datasets & 0/10 & 2.7 & 837.882 & 0.5 & 1 & 0.7 \\ \hline
        spaCy & 6/10 & 2.2 & 610.086 & 1.0 & 1 & 1.4 \\ \hline
        icloud-drive-docker & 0/10 & 2.1 & 127.176 & 0.0 & 0 & 2.8 \\ \hline
    \end{tabular}
    \caption{Full table of results}
    \label{tab:results}
\end{table}

\begin{table}[!h]
    \centering
    \begin{tabular}{|l|l|l|l|l|}
    \hline
        \textbf{Repository} & \textbf{Build Rate} & \textbf{Average \# Attempts} & \textbf{Average Duration (s)} & \textbf{Average \#Relevant} \\ \hline
        qlib & 0/10 & 3.0 & 619.722 & 2 \\ \hline
        cloud-custodian & 0/10 & 2.8 & 176.226 & 1 \\ \hline
        fastapi & 10/10 & 1.0 & 132.549 & 2 \\ \hline
        nonebot2 & 0/10 & 3.0 & 109.311 & 1 \\ \hline
        sabnzbd & 0/10 & 2.6 & 416.891 & 1 \\ \hline
        spotify-downloader & 9/10 & 1.3 & 314.772 & 2 \\ \hline
        sympy & 0/10 & 2.6 & 2869.016 & 2 \\ \hline
        pymc & 0/10 & 2.4 & 238.755 & 2 \\ \hline
        rich & 9/10 & 1.9 & 90.82 & 1 \\ \hline
        mypy & 8/10 & 2.5 & 805.807 & 1 \\ \hline
        scapy & 9/10 & 1.3 & 213.261 & 2 \\ \hline
        aim & 2/10 & 2.8 & 388.659 & 1 \\ \hline
        django-stubs & 8/10 & 2.3 & 150.823 & 1 \\ \hline
        ydata-profiling & 1/10 & 2.8 & 577.774 & 2 \\ \hline
        boto3 & 2/10 & 2.5 & 398.713 & 1 \\ \hline
        textual & 6/10 & 2.5 & 276.669 & 1 \\ \hline
        camel & 0/10 & 2.8 & 520.734 & 3 \\ \hline
        numba & 0/10 & 2.8 & 878.6 & 2 \\ \hline
        black & 3/10 & 2.1 & 466.913 & 1 \\ \hline
        open-interpreter & 10/10 & 1.1 & 74.927 & 1 \\ \hline
        datasets & 0/10 & 3.0 & 1872.915 & 1 \\ \hline
        opencompass & 0/10 & 2.6 & 372.999 & 2 \\ \hline
        scvi-tools & 0/10 & 2.4 & 1548.367 & 1 \\ \hline
        dlt & 0/10 & 2.8 & 325.814 & 1 \\ \hline
        moto & 9/10 & 1.5 & 1211.358 & 3 \\ \hline
        you-get & 7/10 & 2.2 & 185.332 & 1 \\ \hline
        starlette & 10/10 & 1.0 & 72.231 & 2 \\ \hline
        pennylane & 0/10 & 2.6 & 428.72 & 3 \\ \hline
        spaCy & 8/10 & 2.4 & 970.298 & 1 \\ \hline
        speechbrain & 0/10 & 2.8 & 493.776 & 2 \\ \hline
        X-AnyLabeling & 4/10 & 2.2 & 413.028 & 1 \\ \hline
        beets & 3/10 & 2.9 & 136.428 & 1 \\ \hline
        R2R & 0/10 & 2.4 & 109.008 & 1 \\ \hline
        Torch-Pruning & 0/10 & 3.0 & 440.809 & 1 \\ \hline
    \end{tabular}
    \caption{Full table of results for run with search step skipped (perfect recall)}
    \label{tab:results}
\end{table}

\clearpage

\section{Prompts}
For the sake of clarity, the prompts have been split into three groups: the documentation gathering step, Dockerfile generation and Dockerfile repair.
\subsection{Documentation Gathering}
\begin{figure}[h!]
    \begin{lstlisting}
I want to write a dockerfile to build the <REPO_NAME> project.
Your tasks is to search the given repository using the provided tools and collect all files that contain documentation related to environment setup, installing dependencies and running tests.
You musnt try to install the repository using pip. `pip install <REPO_NAME>` is NOT what we want to find, this is INCORRECT and IRRELEVANT to building the project from source, which is our goal.
Such content could be found in files with names like "testing", "contributing", "setup", etc. Collecting an irrelevant file could harm future results, so make sure that any file you register is indeed relevant.
To reiterate, a file that makes explicit mention of **how to install the project's dependencies** or how to **run unit tests** is considered relevant. Anything else should be ignored.
Your focus should be on natural langauge documents. DO NOT attempt to read or register python files, for example.
Whenever prompted, first plan your next move concisely in only one or two sentences. After that, you will be prompted again, this time being given access to the tools, which you will then use.
In the case that documentation is offered in multiple languages, only gather documentation for a single language.

Here are the contents of the repository's root directory (`.`):
<CONTENTS>
    \end{lstlisting}
    \caption{Initial prompt for the documentation gathering step}
\end{figure}
\begin{figure}[h!]
    \begin{lstlisting}
In one or two sentences, give your thoughts about the provided information, then describe what you would like to do next.
You can use provided tools to retrieve additional information about the repo's contents.
Whenever you find a documentation file, you will submit it using the <SUBMIT_TOOL> tool, then continue your search. The whole file is submitted at once, so even if a document contains multiple relevant sections, you only need to submit it once.
Once you are confident that you have found all documentation files in the repository, use the <FINISHED_SEARCH> tool to move on to your next task.
    \end{lstlisting}
    \caption{Follow-up prompt for the documentation gathering step}
\end{figure}

\clearpage
\subsection{Dockerfile Generation}
Believe it or not, the line about being eaten alive in the Dockerfile generation prompt (Figure \ref{fig:dockerfile-gen}) actually helped performance by providing emotional stimulus to the LLM\footnote{https://arxiv.org/abs/2307.11760}.
\begin{figure}[h!]
    \begin{lstlisting}
I want to write a docker file to place inside this repo that will set up a development environment, install any dependencies and run tests to confirm it works.
Remember, instructions such as `pip install <REPO_NAME>` are NOT helpful. I want to build the project from source.
Using the gathered files, collect and summarise any information that may help me. The gatehred files, which you now have access to are:
- <FILES>
You may use the <TOOLS> tools to reinspect the contents of these files if you wish, but you can not access any files other than these. Once you are done, use the <SUMMARISE_TOOL> to give a summary of the information you found.
    \end{lstlisting}
    \caption{Initial prompt for documentation summarisation}
\end{figure}
\begin{figure}[h!]
    \begin{lstlisting}
In one or two sentences, give your thoughts about the provided information, then describe what you would like to do next.
You can use provided tools to retrieve additional information about the repo's contents.
You may use the <TOOLS> tools to reinspect the contents of these files if you wish, but you can not access any files other than these. Once you are done, use the <SUMMARISE_TOOL> to give a summary of the information you found.
    \end{lstlisting}
    \caption{Follow-up prompt for documentation summarisation}
\end{figure}
\begin{figure}[h!]
        \begin{lstlisting}
Now that you have gathered sufficient information about the repository, your new task is write a dockerfile that will be placed inside (<REPO_URL>) and will setup a working environment for the repository. To verify that any cloning and installation succeeds, `RUN` the repo's test suite at the end of the build process.
Here are some tips for successfully writing a dockerfile:
- Since the dockerfile will be placed inside the repository, there is no need to clone the repo!
- To ensure that you dont miss any files important to installing the requirements, make sure to copy the *entirety* of the repo into the container. (`COPY . /app/`)
- make sure that the tests run during the build process, by using `RUN` when running tests. If the final line uses `CMD` then I will be eaten alive (bad). DO NOT EVER USE `CMD <test command>`.
    - Dont add any additional arguments to the test command, that makes it harder for me to identify whether the build was successful.
    - Make sure your final line is `RUN <test command>`

use the `submit_dockerfile` function to provide the dockerfile.
        \end{lstlisting}
        \caption{Prompt for Dockerfile generation}
        \label{fig:dockerfile-gen}
    \end{figure}
\clearpage

\subsection{Dockerfile Repair}
\begin{figure}[h!]
    \begin{lstlisting}
Attempting to build the project using this dockerfile is resulting in the following error:
```
<ERROR_LOG>
```
In one or two sentences only, perform a concise diagnosis of this error. What do the error logs mean, and what could be causing the error? You do not need to suggest a fix to the dockerfile yet, just identify the error.
Some reminders that might be helpful:
<REPAIR_HINTS>
    \end{lstlisting}
    \caption{System prompt for Dockerfile repair}
\end{figure}

\begin{figure}[h!]
    \begin{lstlisting}
- If you are testing with pytest, make sure the final line is `RUN pytest`
- If you are using poetry, then `RUN poetry run pytest`.
- If you are testing with make, the correct command is likely `make test`, not `make tests`.
- trying to copy the requirements file from one place to another will often lead to mistakes. Avoid this too
- Make sure any files that are referenced really do exist. Here are the contents of the repo's root directory, `.` (not including the dockerfile, which you have already been shown):
<ROOT_DIRECTORY>
- It could be the case the repository has more than one requirements file, if (AND ONLY IF) you are certain the repository has multiple requirements files, it could be the case that the requirements from multiple files are needed to run tests.
- If you encounter an error like this: `ERROR: Cannot find command 'git' - do you have 'git' installed and in your PATH?`, then you need to install git like so: `apt install git`
- If you encounter an error that looks like the one shown below, it could be the case that you need to use a newer python version.
```
#9 11.05 ERROR: Cannot install <DEPENDENCY>==<VER_NUM> because these package versions have conflicting dependencies.
#9 11.05 
#9 11.05 The conflict is caused by:
#9 11.05     The user requested <DEPENDENCY>==<VER_NUM>
#9 11.05     The user requested (constraint) <DEPENDENCY>==<VER_NUM>
```
- If you encounter an error such as No module named '<PROJECT_NAME>.xxx' , where the name of the  missing module is also the name of the project you are setting up, then you may need to compile the project before testing.
- You can do this after installing the requirements like so: `pip install wheel ; pip install --no-build-isolation --editable .`
    \end{lstlisting}
    \caption{Additional hints to aid in repair}
\end{figure}

\begin{figure}[h!]
    \begin{lstlisting}
I am trying to build the following dockerfile for the <REPO_URL> repository. It should install any necessary dependencies and then run tests to confirm that installation succeeded:
<DOCKERFILE>
    \end{lstlisting}
    \caption{Prompt for Dockerfile error diagnosis}
\end{figure}

\begin{figure}[h!]
    \begin{lstlisting}
Now, again in only one or two sentences, posit whether this could be fixed by adjusting the dockerfile, or if there is no way for you to build and test this repository.

Examples of errors that cannot be fixed:
- Tests requiring api keys that you do not have access to
- Errors in files in the repo, such as the requirements file.

After explaining the cause of the error, use the provided <READY_TO_FIX> tool to indicate that you are capable of fixing this error by adjusting the dockerfile.
If you already have a fix in mind, use the provided <READY_TO_FIX> tool straight away to confirm that you believe the dockerfile is fixable, and proceed to the next step. Use the <READY_TO_FIX> tool the same way you would any other tool: provide a call to it when you are prompted to use a tool. Do not attempt to use the tool when you are still planning your next move in natural language.
Otherwise (only if you **do not** already have an idea for a fix), you can use the other tools provided (<SEARCH_TOOLS>) to inspect the contents of the repository for additional information. When using these tools, remember to give file paths **relative to the root directory of the project**, absolutely do not include `/usr/src/app/<project_name>` when providing file paths to the search tools.
    \end{lstlisting}
    \caption{Prompt for search stage of Dockerfile error diagnosis}
\end{figure}
\begin{figure}[h!]
    \begin{lstlisting}
You can use the other tools provided (<SEARCH_TOOLS>) to inspect the contents of the repository if you need any additional information. First plan your next action in one or two sentences, then you will be given access to the tools.
After explaining the cause of the error, use the provided <READY_TO_FIX> tool to confirm that you are ready to suggest a fix to the dockerfile.
    \end{lstlisting}
    \caption{Followup prompt for search stage of Dockerfile error diagnosis}
\end{figure}
\begin{figure}[h!]
    \begin{lstlisting}
Now, suggest how to fix this error, then use the provided tool to return a fixed version of the dockerfile.
Recall the tips I gave you earlier:
<REPAIR_HINTS>
    \end{lstlisting}
    \caption{Prompt for generating repaired Dockerfile}
\end{figure}

\end{document}
